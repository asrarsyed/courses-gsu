{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "from sklearn.datasets import load_breast_cancer, load_iris\n",
    "iris = load_iris()\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with EDA before anything else\n",
    "\n",
    "Once you've uploaded a real dataset, your first step should be **EDA (Exploratory Data Analysis)**.  \n",
    "This helps you understand the structure, patterns, and potential issues in your data before diving into modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended workflow: EDA → Feature selection → Train-test split → Scaling → Modeling & evaluation\n",
    "\n",
    "1. **EDA (Exploratory Data Analysis)**  \n",
    "   - Work with the **original, unmodified data**  \n",
    "   - Explore **distributions**, **correlations**, **missing values**, **outliers**, and **class balance**  \n",
    "   - Gain insights that guide the rest of the workflow\n",
    "\n",
    "2. **Feature selection / engineering**  \n",
    "   - Still use the **full dataset** at this stage  \n",
    "   - Create meaningful features, remove highly correlated or irrelevant ones  \n",
    "   - Optionally select top-k features for modeling\n",
    "\n",
    "3. **Train-test split**  \n",
    "   - Divide data into training and testing sets   \n",
    "\n",
    "4. **Scaling (or other transformations)**  \n",
    "   - Fit the scaler **only on the training data**  \n",
    "   - **Apply the same** transformation **to the test set**  \n",
    "\n",
    "5. **Modeling & evaluation**  \n",
    "   - Train your models using the processed training data  \n",
    "   - Use appropriate metrics to assess performance\n",
    "   - At the very end, evaluate them on the transformed test data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why EDA comes first?\n",
    "\n",
    "- You want to explore the **real distribution** of your data.  \n",
    "- Scaling or splitting early might obscure trends (e.g., **class imbalance**, **skewness**).  \n",
    "- Think of EDA as your **diagnostic phase** before doing surgery (modeling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our toy datasets, we will not perform EDA, as the purpose of this demo is to showcase supervised learning models, not exploratory analysis.\n",
    "\n",
    "We will also skip feature engineering and selection, since the dataset already contains a clean and well-structured set of features suitable for modeling.\n",
    "\n",
    "Instead, we will immediately **begin with the train-test split** and proceed directly to modeling and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. EDA → *Almost omitted here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA/t-SNE in EDA vs PCA for preprocessing before modeling\n",
    "\n",
    "\n",
    "#### 1. PCA/t-SNE just for visualization in EDA\n",
    "\n",
    "#### Workflow: EDA (incl. PCA/t-SNE) → Feature selection → Train-test split → Scaling → Modeling & evaluation\n",
    "\n",
    "\n",
    "- Used to **explore structure**, clusters, or class separation\n",
    "- Run on **full dataset** (optionally scaled)\n",
    "- **Does not affect** model training\n",
    "\n",
    "\n",
    "\n",
    "#### 2. PCA for preprocessing before modeling\n",
    "\n",
    "\n",
    "#### Workflow:  EDA (incl. PCA/t-SNE) → Feature selection → Train-test split → Scaling → PCA (fit on train) → Modeling\n",
    "\n",
    "- Used to **reduce dimensionality**\n",
    "- Fit PCA on **training set only**\n",
    "- Transform both train and test\n",
    "- **Part of model input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['Target'] = pd.Categorical.from_codes(cancer.target, categories=cancer.target_names)\n",
    "\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='Target', palette='coolwarm', alpha=0.7).set_title(\"PCA - 2D Projection\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['Dim1', 'Dim2'])\n",
    "df_tsne['Target'] = pd.Categorical.from_codes(cancer.target, categories=cancer.target_names)\n",
    "\n",
    "sns.scatterplot(data=df_tsne, x='Dim1', y='Dim2', hue='Target', palette='coolwarm', alpha=0.7).set_title(\"t-SNE - 2D Projection\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Feature Selection / Engineering → *Omitted*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Binary classification: Breast Cancer\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(X_cancer, y_cancer, test_size=0.3, random_state=42)\n",
    "\n",
    "# Multiclass classification: Iris\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "Xi_train, Xi_test, yi_train, yi_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we split before scaling?\n",
    "\n",
    "When working with machine learning models, it's important to **split your dataset into training and testing sets _before_ applying any scaling** (such as standardization or normalization).\n",
    "\n",
    "If you scale the entire dataset **before splitting**, you're using information from the entire dataset — including the test set — to compute the mean and standard deviation used in scaling.\n",
    "\n",
    "This causes **data leakage**.\n",
    "\n",
    "> ⚠️ **Data leakage** happens when your model gets access to information it shouldn't have during training, leading to overly optimistic results and poor generalization.\n",
    "\n",
    "\n",
    "####  Correct approach: Split → Then Scale\n",
    "\n",
    "The correct workflow is:\n",
    "\n",
    "1. **Split** the dataset into training and test sets.\n",
    "2. **Fit the scaler on the training set** only (i.e., compute mean and std from training data).\n",
    "3. **Transform** both the training and test sets using this fitted scaler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. `StandardScaler`**\n",
    "\n",
    "- **What it does**: Standardizes features by removing the mean and scaling to unit variance (mean = 0, std = 1).\n",
    "- **Best used when**:\n",
    "  - Your features are approximately **normally distributed**\n",
    "  - You want to give equal weight to all features, especially when they are on different scales\n",
    "- **Commonly used with**:\n",
    "  - ✅ Linear Regression\n",
    "  - ✅ Logistic Regression\n",
    "  - ✅ Support Vector Machines (SVM)\n",
    "  - ✅ k-Nearest Neighbors (k-NN)\n",
    "  - ✅ Clustering (e.g., k-Means)\n",
    "  - ❌ Decision Trees / Random Forests (not needed — tree-based models are scale-invariant)\n",
    "  - ❌ Naive Bayes (scaling most likely will not help, as the algorithm relies on distributional assumptions)\n",
    "\n",
    "---\n",
    "\n",
    "**2. `MinMaxScaler`**\n",
    "\n",
    "- **What it does**: Scales features to a fixed range, usually [0, 1].\n",
    "- **Best used when**:\n",
    "  - You want to **preserve the shape** of the original distribution\n",
    "  - Your model is sensitive to the **magnitude of features**\n",
    "  - Your features are **bounded** and **do not contain many outliers**\n",
    "- **Commonly used with**:\n",
    "  - ✅ Logistic Regression\n",
    "  - ✅ SVM\n",
    "  - ✅ k-NN\n",
    "  - ✅ Clustering\n",
    "  - ✅ Neural Networks (especially important here)\n",
    "  - ❌ Decision Trees / Random Forests\n",
    "  - ❌ Naive Bayes\n",
    "\n",
    "---\n",
    "\n",
    "**3. `RobustScaler`**\n",
    "\n",
    "- **What it does**: Scales using the **median** and **interquartile range (IQR)**, making it more robust to outliers.\n",
    "- **Best used when**:\n",
    "  - Your data contains **many outliers**\n",
    "  - You still need scaling for models that assume similar feature scales\n",
    "- **Commonly used with**:\n",
    "  - ✅ Linear Regression (when data has outliers)\n",
    "  - ✅ Logistic Regression\n",
    "  - ✅ SVM\n",
    "  - ✅ k-NN\n",
    "  - ✅ Clustering\n",
    "  - ❌ Decision Trees / Random Forests\n",
    "  - ❌ Naive Bayes\n",
    "\n",
    "---\n",
    "\n",
    "**4. `Normalizer`**\n",
    "\n",
    "- **What it does**: Scales **individual samples (rows)** to have unit norm (i.e., length = 1).\n",
    "- **Best used when**:\n",
    "  - You're working with **sparse data** (e.g., text data like TF-IDF vectors)\n",
    "  - You care about the **direction of vectors**, not their magnitude\n",
    "- **Commonly used with**:\n",
    "  - ✅ k-NN\n",
    "  - ✅ Clustering\n",
    "  - ✅ Text classification\n",
    "  - ❌ Linear/Logistic Regression, SVM \n",
    "  - ❌ Decision Trees / Random Forests\n",
    "  - ❌ Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for common scalers in scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "# StandardScaler: Centers data (mean = 0) and scales to unit variance (std = 1)\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)\n",
    "\n",
    "# MinMaxScaler: Scales features to a defined range, typically [0, 1]\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "# RobustScaler: Uses median and IQR for scaling\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n",
    "\n",
    "# Normalizer: Scales each sample (row) to unit norm (L2 by default)\n",
    "# Useful when you care about the direction of feature vectors, not their magnitude\n",
    "normalizer = Normalizer()\n",
    "X_train_normalized = normalizer.fit_transform(X_train)\n",
    "X_test_normalized = normalizer.transform(X_test)\n",
    "```\n",
    "\n",
    "Each of the transformed datasets (`X_train_standard`, `X_train_minmax`, etc.) can now be used independently for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our datasets, we will use `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize\n",
    "scaler_cancer = StandardScaler()\n",
    "Xc_train_scaled = scaler_cancer.fit_transform(Xc_train)\n",
    "Xc_test_scaled = scaler_cancer.transform(Xc_test)\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "Xi_train_scaled = scaler_iris.fit_transform(Xi_train)\n",
    "Xi_test_scaled = scaler_iris.transform(Xi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using clustering during EDA\n",
    "\n",
    "Clustering is typically considered an unsupervised learning technique, but it can be a powerful tool during **Exploratory Data Analysis (EDA)**.  \n",
    "It helps uncover **natural groupings**, **outliers**, and **hidden structures** in your data before you build predictive models.\n",
    "\n",
    "Using clustering in EDA can surface patterns you wouldn't otherwise see — and can lead to smarter, more informed modeling decisions.\n",
    "\n",
    "**When do we use clustering in EDA?**\n",
    "\n",
    "- If clustering is used for insights only → okay-ish to cluster before the split (but if you want to be completly unbiased do NOT do it!).\n",
    "- If clustering creates a feature → do it after the split, on training data only!\n",
    "\n",
    "**When clustering is useful for supervised learning?**\n",
    "\n",
    "- When you want to understand data structure\n",
    "- To visualize clusters using techniques like PCA or t-SNE\n",
    "- For detecting outliers or unusual subgroups\n",
    "- To engineer new features (e.g., cluster labels for supervised models)\n",
    "\n",
    "\n",
    "\n",
    "**Important considerations**\n",
    "\n",
    "- Do **not** fit clustering models on the test set — only on the training or full EDA set\n",
    "- Apply **feature scaling** (e.g., `StandardScaler`) before clustering, especially for distance-based methods like k-Means\n",
    "- Choose clustering algorithms wisely:\n",
    "  - `KMeans`: Assumes spherical clusters and equal variance\n",
    "  - `GaussianMixture`: More flexible — allows **elliptical clusters** and models **probability** of membership\n",
    "  - `AgglomerativeClustering`: Good for detecting **hierarchical relationships**\n",
    "\n",
    "\n",
    "**You can use clusters as features**\n",
    "\n",
    "Cluster assignments can be saved as a **new categorical feature** and added to your dataset.  \n",
    "This is useful in supervised models, but make sure:\n",
    "- Clusters are created using **training data only**\n",
    "- The process is **replicable** on future (test/real-world) data\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Suggested workflow: EDA → Feature selection → Train-test split → Scaling → Clustering **on training data only** (you can add feature \"Cluster label\") → Modeling & evaluation\n",
    "\n",
    "**If the cluster feature is created on the training data only, how do I use it on the test set later?**\n",
    "\n",
    "You apply the clustering model (fitted on training data) to the test set.\n",
    "Clustering, just like StandardScaler, PCA, or any learned transformation, needs to be:\n",
    "  - Fitted on training data\n",
    "  - Applied (transformed) on test data\n",
    "\n",
    "\n",
    "``` python\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit on training data only\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "train_clusters = kmeans.fit_predict(X_train_scaled)\n",
    "\n",
    "# Add as feature\n",
    "X_train_with_cluster = np.column_stack((X_train_scaled, train_clusters))\n",
    "\n",
    "# Transform test data using THE SAME model\n",
    "test_clusters = kmeans.predict(X_test_scaled)\n",
    "X_test_with_cluster = np.column_stack((X_test_scaled, test_clusters))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Modeling & evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree \n",
    "\n",
    "The `DecisionTreeClassifier` in `scikit-learn` has several key parameters that control how the tree is built and how complex it becomes. Here's a summary of the most important ones:\n",
    "\n",
    "- **criterion**: Specifies the function to measure the quality of a split. Common options include `'gini'` (default) and `'entropy'`.\n",
    "\n",
    "- **max_depth**: The maximum depth of the tree. Limiting depth helps prevent overfitting by controlling how specific the model can become.\n",
    "\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node. Higher values make the model more conservative.\n",
    "\n",
    "- **min_samples_leaf**: The minimum number of samples required to be at a leaf node. This helps smooth the model by avoiding small leaf nodes.\n",
    "\n",
    "- **max_features**: The number of features to consider when looking for the best split. Can be a number, a percentage, or methods like `'sqrt'` or `'log2'`.\n",
    "\n",
    "- **random_state**: Setting this ensures reproducibility across runs.\n",
    "\n",
    "These parameters can be tuned to balance model complexity, accuracy, and generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Binary\n",
    "dt_cancer = DecisionTreeClassifier()\n",
    "dt_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = dt_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"Decision Tree - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass\n",
    "dt_iris = DecisionTreeClassifier()\n",
    "dt_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = dt_iris.predict(Xi_test_scaled)\n",
    "\n",
    "print(\"Decision Tree - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(dt_iris, filled=True, feature_names=iris.feature_names, class_names=iris.target_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(40, 28))  \n",
    "plot_tree(dt_cancer, filled=True, feature_names=cancer.feature_names, class_names=cancer.target_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "The Naive Bayes family of classifiers in `scikit-learn` is based on Bayes’ Theorem and assumes that features are conditionally independent given the class label. These models are simple, fast, and effective, especially for high-dimensional data.\n",
    "\n",
    "### GaussianNB\n",
    "\n",
    "Used for continuous numeric features that are assumed to follow a normal (Gaussian) distribution.\n",
    "\n",
    "- **priors**: Set prior probabilities for each class. If `None`, the priors are learned from the training data.\n",
    "- **var_smoothing**: A small value added to the variances to prevent division by zero and improve numerical stability.\n",
    "\n",
    "### MultinomialNB\n",
    "\n",
    "Used for discrete count data, such as word counts in text classification problems.\n",
    "\n",
    "- **alpha**: Additive (Laplace/Lidstone) smoothing parameter. Helps handle features that don’t appear in the training data.\n",
    "- **fit_prior**: Whether to learn class prior probabilities from the training data. If `False`, uniform priors are used.\n",
    "- **class_prior**: Manually specify class priors if `fit_prior=False`.\n",
    "\n",
    "### BernoulliNB\n",
    "\n",
    "Used for binary/boolean features, where each feature is either present or absent (1 or 0).\n",
    "\n",
    "- **alpha**: Smoothing parameter, similar to `MultinomialNB`.\n",
    "- **binarize**: Threshold for converting feature values to binary (0 or 1). Default is `0.0`, meaning any positive value becomes 1.\n",
    "- **fit_prior** and **class_prior**: Same behavior as in `MultinomialNB`.\n",
    "\n",
    "Each variant of Naive Bayes is suited for different types of input features. Despite the naive assumption of feature independence, these models often perform well in practice and are especially useful as a baseline for classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Binary\n",
    "nb_cancer = GaussianNB()\n",
    "nb_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = nb_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"Naive Bayes - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass\n",
    "nb_iris = GaussianNB()\n",
    "nb_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = nb_iris.predict(Xi_test_scaled)\n",
    "print(\"Naive Bayes - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "The `KNeighborsClassifier` in `scikit-learn` is a simple, non-parametric method that classifies a data point based on the majority class among its *k* nearest neighbors in the feature space.\n",
    "\n",
    "Here are the key parameters that control how the model behaves:\n",
    "\n",
    "- **n_neighbors**: The number of nearest neighbors to consider (i.e., the \"k\" in KNN). Increasing this value can make the model more stable but less sensitive to local patterns.\n",
    "\n",
    "- **weights**: Determines how to weight the contribution of neighbors. Options include:\n",
    "  - `'uniform'`: All neighbors are weighted equally (default).\n",
    "  - `'distance'`: Closer neighbors have a greater influence.\n",
    "\n",
    "- **algorithm**: The algorithm used to compute nearest neighbors. Options include `'auto'`, `'ball_tree'`, `'kd_tree'`, and `'brute'`. `'auto'` chooses the best method based on the data.\n",
    "\n",
    "- **metric**: The distance metric used to find neighbors. Common options include `'minkowski'`, `'euclidean'`, and `'manhattan'`.\n",
    "\n",
    "- **p**: Power parameter for the Minkowski metric. When `p=1`, it is equivalent to Manhattan distance; when `p=2`, it becomes Euclidean distance.\n",
    "\n",
    "KNN is intuitive and often effective for smaller datasets, but it can become computationally expensive with large datasets or high-dimensional feature spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Binary\n",
    "knn_cancer = KNeighborsClassifier()\n",
    "knn_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = knn_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"KNN - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass\n",
    "knn_iris = KNeighborsClassifier()\n",
    "knn_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = knn_iris.predict(Xi_test_scaled)\n",
    "\n",
    "print(\"KNN - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "The `LogisticRegression` classifier in `scikit-learn` is a linear model used for binary and multiclass classification. It models the probability of class membership using a logistic (sigmoid) function and finds the best-fitting linear decision boundary.\n",
    "\n",
    "Below are the key parameters that control its behavior:\n",
    "\n",
    "- **penalty**: Specifies the type of regularization to apply. Common options include:\n",
    "  - `'l2'` (default): Ridge regularization.\n",
    "  - `'l1'`: Lasso regularization (only supported with certain solvers).\n",
    "  - `'elasticnet'`: Combination of L1 and L2.\n",
    "  - `'none'`: No regularization.\n",
    "\n",
    "- **C**: Inverse of regularization strength. Smaller values imply stronger regularization. It helps control overfitting.\n",
    "\n",
    "- **solver**: The algorithm used to optimize the model. Common options include:\n",
    "  - `'lbfgs'` (default, good for small to medium data).\n",
    "  - `'liblinear'` (works with L1 and L2 penalties).\n",
    "  - `'saga'` (supports L1, L2, and elasticnet for large datasets).\n",
    "\n",
    "- **max_iter**: Maximum number of iterations taken for the solver to converge. Increase this if the model doesn't converge with the default setting.\n",
    "\n",
    "- **multi_class**: Strategy for multiclass problems. Options include:\n",
    "  - `'auto'` (default): Chooses `'ovr'` or `'multinomial'` based on solver.\n",
    "  - `'ovr'`: One-vs-Rest.\n",
    "  - `'multinomial'`: Multiclass optimization.\n",
    "\n",
    "- **random_state**: Ensures reproducibility by setting the seed for random number generation.\n",
    "\n",
    "Logistic Regression is a robust and interpretable model that works well when the relationship between features and the log-odds of the outcome is linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary\n",
    "lr_cancer = LogisticRegression(max_iter=1000)\n",
    "lr_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = lr_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass\n",
    "lr_iris = LogisticRegression(max_iter=1000)\n",
    "lr_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = lr_iris.predict(Xi_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "model.fit(Xc_train_scaled, yc_train)\n",
    "\n",
    "feature_names = cancer.feature_names  \n",
    "coefs = model.coef_.ravel()  \n",
    "eliminated = feature_names[coefs == 0]\n",
    "selected = feature_names[coefs != 0]\n",
    "\n",
    "print(\"Eliminated features:\")\n",
    "print(eliminated)\n",
    "\n",
    "print(\"\\nSelected features:\")\n",
    "print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression with L1 for multiclass (use 'saga' solver with multinomial loss)\n",
    "model = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    C=1.0,\n",
    "    max_iter=5000\n",
    ")\n",
    "model.fit(Xi_train_scaled, yi_train)\n",
    "\n",
    "feature_names = iris.feature_names  \n",
    "coefs = model.coef_  # Get coefficients matrix: shape (n_classes, n_features)\n",
    "\n",
    "used_mask = (coefs != 0).any(axis=0)  # True for selected features\n",
    "eliminated = np.array(feature_names)[~used_mask]\n",
    "selected = np.array(feature_names)[used_mask]\n",
    "\n",
    "print(\"Eliminated features (all-zero across all classes):\")\n",
    "print(eliminated)\n",
    "\n",
    "print(\"\\nSelected features (used by at least one class):\")\n",
    "print(selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing the Decision Threshold in Logistic Regression\n",
    "\n",
    "By default, `LogisticRegression` uses a **0.5 threshold** to make class predictions. This means that a sample is classified into a class if its predicted probability is **≥ 0.5**. However, in many real-world scenarios, it can be useful to **adjust this threshold** to better control the trade-off between **precision** and **recall**.\n",
    "\n",
    "\n",
    "####  Why Adjust the Threshold?\n",
    "\n",
    "- **Imbalanced classes**: In cases like fraud detection or medical diagnosis, the positive class is rare. Lowering the threshold can increase the sensitivity (recall).\n",
    "- **Business or application priorities**: You may prefer to avoid false negatives or false positives depending on the context.\n",
    "- **More control over model behavior**: Especially useful for fine-tuning performance beyond what accuracy alone can provide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "probs = lr_cancer.predict_proba(Xc_test_scaled)\n",
    "\n",
    "# Use custom threshold (e.g., 0.3)\n",
    "threshold = 0.3\n",
    "custom_preds = (probs[:, 1] >= threshold).astype(int)\n",
    "print(classification_report(yc_test, custom_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = lr_iris.predict_proba(Xi_test_scaled)\n",
    "threshold = 0.6\n",
    "\n",
    "# Assign class only if highest probability ≥ threshold\n",
    "max_probs = np.max(probs, axis=1)\n",
    "pred_classes = np.argmax(probs, axis=1)\n",
    "custom_preds = np.where(max_probs >= threshold, pred_classes, -1)  # -1 = uncertain\n",
    "print(classification_report(yi_test, custom_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "The `SVC` classifier in `scikit-learn` is a powerful model that finds the optimal hyperplane to separate classes in the feature space. It works well for both linear and non-linear classification tasks and can handle high-dimensional data effectively.\n",
    "\n",
    "Below are the main parameters to control an SVM model:\n",
    "\n",
    "- **kernel**: Specifies the kernel type used to transform the data. Common options include:\n",
    "  - `'linear'`: Linear decision boundary.\n",
    "  - `'rbf'` (default): Radial Basis Function, useful for non-linear problems.\n",
    "  - `'poly'`: Polynomial kernel.\n",
    "  - `'sigmoid'`: Used less frequently.\n",
    "\n",
    "- **C**: Regularization parameter. A smaller value allows a softer margin (more tolerance for misclassification), while a larger value tries to fit the training data more strictly.\n",
    "\n",
    "- **gamma**: Controls the influence of individual data points for non-linear kernels (`'rbf'`, `'poly'`, `'sigmoid'`). \n",
    "  - `'scale'` (default) or `'auto'` are common settings.\n",
    "  - Higher values lead to tighter decision boundaries (can overfit).\n",
    "\n",
    "- **degree**: Degree of the polynomial kernel function (used only if `kernel='poly'`).\n",
    "\n",
    "- **probability**: If set to `True`, enables probability estimates via cross-validation. This makes the model slower but useful when you need class probabilities.\n",
    "\n",
    "- **random_state**: Controls the shuffling for probability estimates and reproducibility.\n",
    "\n",
    "SVMs are effective in high-dimensional spaces and are versatile due to the use of different kernel functions. They can be sensitive to parameter settings, so tuning `C`, `gamma`, and `kernel` is often necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Binary - Linear\n",
    "svm_linear_cancer = SVC(kernel='linear')\n",
    "svm_linear_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = svm_linear_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"SVM (Linear) - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary - RBF\n",
    "svm_rbf_cancer = SVC(kernel='rbf')\n",
    "svm_rbf_cancer.fit(Xc_train_scaled, yc_train)\n",
    "yc_pred = svm_rbf_cancer.predict(Xc_test_scaled)\n",
    "\n",
    "print(\"SVM (RBF) - Breast Cancer\")\n",
    "print(classification_report(yc_test, yc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass - Linear\n",
    "svm_linear_iris = SVC(kernel='linear')\n",
    "svm_linear_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = svm_linear_iris.predict(Xi_test_scaled)\n",
    "\n",
    "print(\"SVM (Linear) - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass - RBF\n",
    "svm_rbf_iris = SVC(kernel='rbf')\n",
    "svm_rbf_iris.fit(Xi_train_scaled, yi_train)\n",
    "yi_pred = svm_rbf_iris.predict(Xi_test_scaled)\n",
    "\n",
    "print(\"SVM (RBF) - Iris\")\n",
    "print(classification_report(yi_test, yi_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
